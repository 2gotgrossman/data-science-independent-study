+ Structure of a neural network
    + Fully Connected
      * Their theoretical power --> Function Approximator
      * Their conceptual power --> Create your own features!
      * Why Neural?
          + Hierarchical model
          + Activation function
+ Activation Functions
    + Non-linear activation functions
+ The Process
    + Feedforward
    * Backpropagation
        + The computational expense of training
    * Gradient Descent
        + Local minima, local minima everywhere
+ Deep Learning
+ Convolutional NN
    * Convolutions --> Exploit similarities in image
    * Pooling --> Reduce # of weights
    * Parameters --> Stride, size of kernels, number of filters, etc
    * AlexNet
+ Recurrent NN
    * LSTMs
    * Memory
+ The Tricks to Learning
    * Vanishing gradients
    * Learning rates
    * Dropout
    * Mini-Batches
+ Reinforcement Learning
+ Autoencoders
+ Unsupervised ML
